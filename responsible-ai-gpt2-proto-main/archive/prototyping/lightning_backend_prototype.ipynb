{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T12:03:55.408346Z",
     "start_time": "2023-04-24T12:03:55.398194Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T12:03:55.409137Z",
     "start_time": "2023-04-24T12:03:55.402964Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GPT2Dataset(Dataset):\n",
    "    def __init__(self, tokenizer, texts, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"].squeeze()\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
    "        \n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T12:03:55.409939Z",
     "start_time": "2023-04-24T12:03:55.396878Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.adamw import AdamW\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create a PyTorch Lightning module that wraps the GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T12:03:55.417915Z",
     "start_time": "2023-04-24T12:03:55.398818Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GPT2FineTuner(pl.LightningModule):\n",
    "    def __init__(self, model, tokenizer, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask = batch\n",
    "\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        loss = outputs[0].mean()\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=1e-5)\n",
    "\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load the pre-trained GPT-2 model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T12:03:55.981308Z",
     "start_time": "2023-04-24T12:03:55.400967Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpt_config = AutoConfig.from_pretrained(\"sshleifer/tiny-gpt2\", output_hidden_states=True)\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/tiny-gpt2\", config=gpt_config)\n",
    "gpt_model = AutoModel.from_pretrained(\"sshleifer/tiny-gpt2\", config=gpt_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create an instance of the GPT2FineTuner and a PyTorch Lightning Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T12:03:56.021176Z",
     "start_time": "2023-04-24T12:03:55.402824Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fine_tuner = GPT2FineTuner(gpt_model, gpt_tokenizer, gpt_config)\n",
    "trainer = pl.Trainer(max_epochs=200, enable_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T12:03:56.023797Z",
     "start_time": "2023-04-24T12:03:55.402908Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace this with your list of texts\n",
    "texts = [\"Text 1\", \"Text 2\", \"Text 3\"]\n",
    "\n",
    "# Create a dataset instance\n",
    "dataset = GPT2Dataset(gpt_tokenizer, texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Prepare your dataset using Hugging Face's Dataset library or custom data loader, and then start the fine-tuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T12:04:07.785128Z",
     "start_time": "2023-04-24T12:03:55.404983Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "trainer.fit(fine_tuner, train_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
