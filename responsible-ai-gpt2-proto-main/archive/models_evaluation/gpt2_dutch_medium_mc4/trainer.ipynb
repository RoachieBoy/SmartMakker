{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-13T22:41:32.999728Z",
     "end_time": "2023-04-13T22:41:35.527011Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import warnings \n",
    "\n",
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.aitextgen import aitextgen\n",
    "\n",
    "model_source = \"yhavinga/gpt2-medium-dutch\"\n",
    "filepath_dataset = r\"../evaluation_training_data/lyrics_2500.csv\"\n",
    "num_steps = 7500\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-13T22:41:35.528011Z",
     "end_time": "2023-04-13T22:41:35.541014Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpu_available = torch.cuda.is_available()\n",
    "\n",
    "if gpu_available:\n",
    "    torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-13T22:41:35.541014Z",
     "end_time": "2023-04-13T22:41:40.147047Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at aitextgen\\models--yhavinga--gpt2-medium-dutch\\snapshots\\b22286e498f82d4906dc06a617dea2d0b3b10fa3\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"yhavinga/gpt2-medium-dutch\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.0,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.27.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at aitextgen\\models--yhavinga--gpt2-medium-dutch\\snapshots\\b22286e498f82d4906dc06a617dea2d0b3b10fa3\\pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.27.4\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at yhavinga/gpt2-medium-dutch.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "loading file vocab.json from cache at C:\\Users\\winan/.cache\\huggingface\\hub\\models--yhavinga--gpt2-medium-dutch\\snapshots\\b22286e498f82d4906dc06a617dea2d0b3b10fa3\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\winan/.cache\\huggingface\\hub\\models--yhavinga--gpt2-medium-dutch\\snapshots\\b22286e498f82d4906dc06a617dea2d0b3b10fa3\\merges.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\winan/.cache\\huggingface\\hub\\models--yhavinga--gpt2-medium-dutch\\snapshots\\b22286e498f82d4906dc06a617dea2d0b3b10fa3\\tokenizer.json\n",
      "loading file added_tokens.json from cache at C:\\Users\\winan/.cache\\huggingface\\hub\\models--yhavinga--gpt2-medium-dutch\\snapshots\\b22286e498f82d4906dc06a617dea2d0b3b10fa3\\added_tokens.json\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\winan/.cache\\huggingface\\hub\\models--yhavinga--gpt2-medium-dutch\\snapshots\\b22286e498f82d4906dc06a617dea2d0b3b10fa3\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\winan/.cache\\huggingface\\hub\\models--yhavinga--gpt2-medium-dutch\\snapshots\\b22286e498f82d4906dc06a617dea2d0b3b10fa3\\tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "model = aitextgen(\n",
    "    model=model_source,\n",
    "    verbose=True,\n",
    "    to_gpu=gpu_available,\n",
    ")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_source)\n",
    "\n",
    "model.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-13T22:41:40.144049Z",
     "end_time": "2023-04-13T22:41:40.160052Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 1024)\n    (wpe): Embedding(1024, 1024)\n    (drop): Dropout(p=0.0, inplace=False)\n    (h): ModuleList(\n      (0-23): 24 x GPT2Block(\n        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-13T22:41:40.162052Z",
     "end_time": "2023-04-13T22:41:40.701172Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/2507 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38f8a9fac65649c3a1300d90ea1503ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = TokenDataset(\n",
    "    file_path=filepath_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    line_by_line=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-13T22:41:42.300440Z",
     "end_time": "2023-04-13T23:12:43.374086Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Windows does not support multi-GPU training. Setting to 1 GPU.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/7500 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8bfd2e4c06a54ae694221489c3450a55"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.27.4\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m750 steps reached: generating sample texts.\u001B[0m\n",
      "==========\n",
      "niemand die nog buiten staat\n",
      "ik heb de deur voor het raam\n",
      "maar ik ben bang dat ik nu blijf\n",
      "\n",
      "en wat ik nu ook doe\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "niemand die nog buiten staat\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./output\\config.json\n",
      "Configuration saved in ./output\\generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1,000 steps reached: saving model to /./output\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./output\\pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.27.4\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1,500 steps reached: generating sample texts.\u001B[0m\n",
      "==========\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga terug\n",
      "ik ga\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./output\\config.json\n",
      "Configuration saved in ./output\\generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m2,000 steps reached: saving model to /./output\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./output\\pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.27.4\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m2,250 steps reached: generating sample texts.\u001B[0m\n",
      "==========\n",
      "je bent mn baby\n",
      "je bent mn beste vriend\n",
      "en ik wil dat je blijft\n",
      "\n",
      "geef me je handen en laat me je dromen\n",
      "zeg me dat je van me houdt\n",
      "\n",
      "geef me je ogen\n",
      "en laat me je dromen\n",
      "zeg me dat je van me houdt\n",
      "\n",
      "jij en ik\n",
      "wij zijn een ja en een kruisje\n",
      "jij en ik\n",
      "wij zijn een ja en een kruisje\n",
      "jij en ik\n",
      "wij zijn een ja en een kruisje\n",
      "jij en ik\n",
      "\n",
      "jij en ik\n",
      "wij zijn een ja en een kruisje\n",
      "jij en ik\n",
      "\n",
      "wij zijn een ja en een kruisje\n",
      "jij en ik\n",
      "wij zijn een ja en een kruisje\n",
      "jij en ik\n",
      "\n",
      "jij en ik\n",
      "wij zijn een ja en een kruisje\n",
      "jij en ik\n",
      "jij en ik\n",
      "wij zijn een ja en een kruisje\n",
      "jij en ik\n",
      "jij en ik\n",
      "jij en ik\n",
      "jij en ik\n",
      "jij en ik\n",
      "jij en ik\n",
      "jij en ik\n",
      "jij en ik\n",
      "jij en ik\n",
      "\n",
      "jij en ik\n",
      "jij en ik\n",
      "jij en ik\n",
      "jij en ik\n",
      "jij en ik\n",
      "jij en ik\n",
      "jij en ik\n",
      "jij en ik\n",
      "jij en ik\n",
      "jij en ik\n",
      "jij en ik\n",
      "jij en wij\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./output\\config.json\n",
      "Configuration saved in ./output\\generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m3,000 steps reached: saving model to /./output\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./output\\pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.27.4\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m3,000 steps reached: generating sample texts.\u001B[0m\n",
      "==========\n",
      "\n",
      "songtekst van maan  wat jij wilt\n",
      "\n",
      "\n",
      "jij hoeft niets meer te vragen\n",
      "ik sta naast je\n",
      "wat heb jij mij gebracht\n",
      "wat hoef ik nou zonder jou te beginnen\n",
      "\n",
      "een nieuwe dag\n",
      "morgen weer vroeg opgestaan\n",
      "een nieuwe morgen\n",
      "\n",
      "morgen ben jij m weer vergeten\n",
      "maar de dag daarna\n",
      "morgen ben jij er weer\n",
      "en wat heb ik dan zonder jou\n",
      "nog geen zin\n",
      "ik ga gewoon mijn eigen gang\n",
      "morgen ben jij weer vroeg opgestaan\n",
      "morgen ben jij weer vroeg opgestaan\n",
      "\n",
      "ik weet wat jij wilt\n",
      "ik weet wat ik voelen moet\n",
      "het is nu echt nog niet te laat\n",
      "toe luister nu wat ik zeg\n",
      "dit is wat ik jou nu zeg\n",
      "wat jij nog niet had\n",
      "\n",
      "morgen ben jij m weer vergeten\n",
      "maar de dag daarna\n",
      "morgen ben jij er weer\n",
      "en wat heb ik dan zonder jou\n",
      "nog geen zin\n",
      "ik ga gewoon mijn eigen gang\n",
      "morgen ben jij m weer vergeten\n",
      "morgen ben jij er weer\n",
      "en wat heb ik dan zonder jou\n",
      "nog geen zin\n",
      "ik ga gewoon mijn eigen gang\n",
      "morgen ben jij m weer vergeten\n",
      "maar de dag erna\n",
      "morgen ben jij er weer\n",
      "en wat heb ik dan zonder jou\n",
      "nog geen zin\n",
      "ik ga gewoon mijn eigen\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.27.4\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m3,750 steps reached: generating sample texts.\u001B[0m\n",
      "==========\n",
      "\n",
      "want ik zag je naar me kijken en toen wist ik al het is me gelukt oh yeah\n",
      "want ik ga straks naar huis met het meisje van de hockeyclub\n",
      "\n",
      "annefleur of willemijn\n",
      "is dit waar we willen zijn ahah ahah\n",
      "want ik heb shotjes op de tafel staan hier\n",
      "en deze nacht die is van mij\n",
      "hoe lang sta je al op mn lijstje\n",
      "\n",
      "je hebt me al zo vaak gezien laten zien\n",
      "waarom ben je niet meer hier\n",
      "je hebt een lovely body\n",
      "je bent het mooiste dat er is\n",
      "je bent het helemaal waar kan alles zijn\n",
      "ja ik ben zo van streek maar weet je\n",
      "\n",
      "je hebt me al zo vaak gezien laten zien\n",
      "waarom ben je niet meer hier\n",
      "je hebt een lovely body\n",
      "je bent het mooiste dat er is\n",
      "je bent het helemaal waar kan alles zijn\n",
      "ja ik ben zo van streek maar weet je\n",
      "\n",
      "je hebt me al zo vaak gezien laten zien\n",
      "waarom ben je niet meer hier\n",
      "je hebt een lovely body\n",
      "je bent het mooiste dat er is\n",
      "je bent het helemaal waar kan alles zijn\n",
      "ja ik ben zo van streek maar weet je\n",
      "\n",
      "je hebt me al zo vaak gezien laten zien\n",
      "waarom ben\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./output\\config.json\n",
      "Configuration saved in ./output\\generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m4,000 steps reached: saving model to /./output\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./output\\pytorch_model.bin\n",
      "Configuration saved in ./output\\config.json\n",
      "Configuration saved in ./output\\generation_config.json\n",
      "Model weights saved in ./output\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    train_data=train_data,\n",
    "    seed=27,\n",
    "    num_steps=num_steps,\n",
    "    generate_every=750,\n",
    "    output_dir=r\"./output\",\n",
    "    freeze_layers=True,\n",
    "    num_layers_freeze=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
