{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T22:21:24.568408Z",
     "end_time": "2023-04-11T22:21:26.860618Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import warnings \n",
    "\n",
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.aitextgen import aitextgen\n",
    "\n",
    "model_source = \"yhavinga/gpt-neo-125M-dutch\"\n",
    "filepath_dataset = r\"../evaluation_training_data/lyrics_400.csv\"\n",
    "num_steps = 200\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T22:21:38.354923Z",
     "end_time": "2023-04-11T22:21:38.382928Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpu_available = torch.cuda.is_available()\n",
    "\n",
    "if gpu_available:\n",
    "    torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T22:21:40.505719Z",
     "end_time": "2023-04-11T22:21:42.887427Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at aitextgen\\models--yhavinga--gpt-neo-125M-dutch\\snapshots\\f7ba70ce7b62fbd1c298fd9012cf7b3b9bf0fd5d\\config.json\n",
      "Model config GPTNeoConfig {\n",
      "  \"_name_or_path\": \"yhavinga/gpt-neo-125M-dutch\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      6\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0.0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.27.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at aitextgen\\models--yhavinga--gpt-neo-125M-dutch\\snapshots\\f7ba70ce7b62fbd1c298fd9012cf7b3b9bf0fd5d\\pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.27.4\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPTNeoForCausalLM.\n",
      "\n",
      "All the weights of GPTNeoForCausalLM were initialized from the model checkpoint at yhavinga/gpt-neo-125M-dutch.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoForCausalLM for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "loading file vocab.json from cache at aitextgen\\models--yhavinga--gpt-neo-125M-dutch\\snapshots\\f7ba70ce7b62fbd1c298fd9012cf7b3b9bf0fd5d\\vocab.json\n",
      "loading file merges.txt from cache at aitextgen\\models--yhavinga--gpt-neo-125M-dutch\\snapshots\\f7ba70ce7b62fbd1c298fd9012cf7b3b9bf0fd5d\\merges.txt\n",
      "loading file tokenizer.json from cache at aitextgen\\models--yhavinga--gpt-neo-125M-dutch\\snapshots\\f7ba70ce7b62fbd1c298fd9012cf7b3b9bf0fd5d\\tokenizer.json\n",
      "loading file added_tokens.json from cache at aitextgen\\models--yhavinga--gpt-neo-125M-dutch\\snapshots\\f7ba70ce7b62fbd1c298fd9012cf7b3b9bf0fd5d\\added_tokens.json\n",
      "loading file special_tokens_map.json from cache at aitextgen\\models--yhavinga--gpt-neo-125M-dutch\\snapshots\\f7ba70ce7b62fbd1c298fd9012cf7b3b9bf0fd5d\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at aitextgen\\models--yhavinga--gpt-neo-125M-dutch\\snapshots\\f7ba70ce7b62fbd1c298fd9012cf7b3b9bf0fd5d\\tokenizer_config.json\n",
      "loading file vocab.json from cache at C:\\Users\\winan/.cache\\huggingface\\hub\\models--yhavinga--gpt-neo-125M-dutch\\snapshots\\f7ba70ce7b62fbd1c298fd9012cf7b3b9bf0fd5d\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\winan/.cache\\huggingface\\hub\\models--yhavinga--gpt-neo-125M-dutch\\snapshots\\f7ba70ce7b62fbd1c298fd9012cf7b3b9bf0fd5d\\merges.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\winan/.cache\\huggingface\\hub\\models--yhavinga--gpt-neo-125M-dutch\\snapshots\\f7ba70ce7b62fbd1c298fd9012cf7b3b9bf0fd5d\\tokenizer.json\n",
      "loading file added_tokens.json from cache at C:\\Users\\winan/.cache\\huggingface\\hub\\models--yhavinga--gpt-neo-125M-dutch\\snapshots\\f7ba70ce7b62fbd1c298fd9012cf7b3b9bf0fd5d\\added_tokens.json\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\winan/.cache\\huggingface\\hub\\models--yhavinga--gpt-neo-125M-dutch\\snapshots\\f7ba70ce7b62fbd1c298fd9012cf7b3b9bf0fd5d\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\winan/.cache\\huggingface\\hub\\models--yhavinga--gpt-neo-125M-dutch\\snapshots\\f7ba70ce7b62fbd1c298fd9012cf7b3b9bf0fd5d\\tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "model = aitextgen(\n",
    "    model=model_source,\n",
    "    verbose=True,\n",
    "    to_gpu=gpu_available,\n",
    ")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_source)\n",
    "\n",
    "model.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T22:21:52.601431Z",
     "end_time": "2023-04-11T22:21:52.634439Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "GPTNeoForCausalLM(\n  (transformer): GPTNeoModel(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(2048, 768)\n    (drop): Dropout(p=0.0, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPTNeoBlock(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPTNeoAttention(\n          (attention): GPTNeoSelfAttention(\n            (attn_dropout): Dropout(p=0.0, inplace=False)\n            (resid_dropout): Dropout(p=0.0, inplace=False)\n            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPTNeoMLP(\n          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/396 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a7d6f744107415392baa9ad8f9d77b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = TokenDataset(\n",
    "    file_path=filepath_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    line_by_line=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T22:21:53.514318Z",
     "end_time": "2023-04-11T22:21:53.605338Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Windows does not support multi-GPU training. Setting to 1 GPU.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/200 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2a3386691514c199f7a7fd0a79c4dbf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.27.4\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m100 steps reached: generating sample texts.\u001B[0m\n",
      "==========\n",
      "\n",
      "\n",
      " je nog al de dromen dagen die zon\n",
      "het het leven\n",
      "er van de zon\n",
      "ook de leven\n",
      "het was\n",
      "\n",
      "dat zijn met jouw leven\n",
      "ja in die leven\n",
      "de leven\n",
      "de zon\n",
      "ja je leven\n",
      "daar bent\n",
      "het leven\n",
      "daar is\n",
      "daar zou je niet meer\n",
      "voor de zon zijn\n",
      "ik wil mijn mensen zijn vakantie leven\n",
      "ga maar meer niet meer\n",
      "het leven\n",
      "\n",
      "\n",
      "de hand\n",
      "\n",
      "daar heeft het zijn\n",
      "\n",
      "de hele dag\n",
      "het leven\n",
      "\n",
      "wij\n",
      "de liefde\n",
      "na leven\n",
      "het leven\n",
      "het leven\n",
      "als je nog meer en die je geen leven maar maar dan van de zon\n",
      "het was\n",
      "het zijn\n",
      "ik wil je niet\n",
      "ik ga je van de zon\n",
      "daar is\n",
      "laat het leven\n",
      "daar had\n",
      "het wil ik je leven\n",
      "toen de wereld\n",
      "ik zijn\n",
      "daar is hier zijn\n",
      "toen is ik je leven\n",
      "laat is zijn van de zon\n",
      "daar kwam\n",
      "zo wil je leven\n",
      "het leven\n",
      "\n",
      "je blijft nog aan\n",
      "en ik de liefde\n",
      "laat je\n",
      "ik wil de zon\n",
      "ook hier meer\n",
      "de leven zijn\n",
      "ik wil\n",
      "het leven\n",
      "\n",
      "daar is hier\n",
      "geen leven\n",
      "ik hou je leven\n",
      "ik wil zijn\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.27.4\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m200 steps reached: generating sample texts.\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=200` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "verapapala\n",
      "\n",
      "ik zal niet te geven\n",
      "wat is van jou nog het is het is hier kan je liefde\n",
      "t is het wel al jou\n",
      "een leven\n",
      "de ene oude naam\n",
      "en die tijd en liefde\n",
      "\n",
      "zeg in de weg\n",
      "dus moet je bent een meisje\n",
      "maar voor jou je je bent\n",
      "daar je bij jou\n",
      "ik wil je weet je mij\n",
      "\n",
      "jij bent nooit weer blij\n",
      "\n",
      "is alles\n",
      "dat is de tijd\n",
      "je geeft en ga naar jou\n",
      "dat jij niet de zomer en jij een kind\n",
      "maar\n",
      "ealie\n",
      "\n",
      "\n",
      "jij bent jij\n",
      "ja\n",
      "\n",
      "ja die ik je geeft maar die liefde van jou\n",
      "je bent met jou\n",
      "t is\n",
      "jij bent niet vinden\n",
      "n huis is\n",
      "wat\n",
      "jij bent het is hier al de ene vrouw en ik ben jij de liefde\n",
      "tot de weg voor jou\n",
      "jij bent dat je mee\n",
      "\n",
      "de liefde\n",
      "je bent\n",
      "met jou voor jou\n",
      "het is voor jou voor jou\n",
      "diep\n",
      "\n",
      "je bent je van je bent naar jou\n",
      "de tijd\n",
      "\n",
      "\n",
      "maar een ander\n",
      "\n",
      "die een lach van jouw liefde\n",
      "maar nog blij\n",
      "dat is niet meer\n",
      "\n",
      "het is wat jij het was jij me is niet te zijn\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./output\\config.json\n",
      "Configuration saved in ./output\\generation_config.json\n",
      "Model weights saved in ./output\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    train_data=train_data,\n",
    "    seed=27,\n",
    "    num_steps=num_steps,\n",
    "    generate_every=100,\n",
    "    output_dir=r\"./output\",\n",
    "    freeze_layers=True,\n",
    "    num_layers_to_freeze=10,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T22:21:56.078865Z",
     "end_time": "2023-04-11T22:23:20.559772Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
