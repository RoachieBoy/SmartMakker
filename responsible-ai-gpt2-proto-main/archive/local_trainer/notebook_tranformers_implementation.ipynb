{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, Trainer, TrainingArguments, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_repo = \"yhavinga/gpt-neo-125m-dutch\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_repo)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_repo, config=config)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_repo, config=config)\n",
    "\n",
    "if config.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = config.eos_token_id\n",
    "else:\n",
    "    tokenizer.pad_token_id = config.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wte.weight\n",
      "wpe.weight\n",
      "h.0.ln_1.weight\n",
      "h.0.ln_1.bias\n",
      "h.0.attn.attention.k_proj.weight\n",
      "h.0.attn.attention.v_proj.weight\n",
      "h.0.attn.attention.q_proj.weight\n",
      "h.0.attn.attention.out_proj.weight\n",
      "h.0.attn.attention.out_proj.bias\n",
      "h.0.ln_2.weight\n",
      "h.0.ln_2.bias\n",
      "h.0.mlp.c_fc.weight\n",
      "h.0.mlp.c_fc.bias\n",
      "h.0.mlp.c_proj.weight\n",
      "h.0.mlp.c_proj.bias\n",
      "h.1.ln_1.weight\n",
      "h.1.ln_1.bias\n",
      "h.1.attn.attention.k_proj.weight\n",
      "h.1.attn.attention.v_proj.weight\n",
      "h.1.attn.attention.q_proj.weight\n",
      "h.1.attn.attention.out_proj.weight\n",
      "h.1.attn.attention.out_proj.bias\n",
      "h.1.ln_2.weight\n",
      "h.1.ln_2.bias\n",
      "h.1.mlp.c_fc.weight\n",
      "h.1.mlp.c_fc.bias\n",
      "h.1.mlp.c_proj.weight\n",
      "h.1.mlp.c_proj.bias\n",
      "h.2.ln_1.weight\n",
      "h.2.ln_1.bias\n",
      "h.2.attn.attention.k_proj.weight\n",
      "h.2.attn.attention.v_proj.weight\n",
      "h.2.attn.attention.q_proj.weight\n",
      "h.2.attn.attention.out_proj.weight\n",
      "h.2.attn.attention.out_proj.bias\n",
      "h.2.ln_2.weight\n",
      "h.2.ln_2.bias\n",
      "h.2.mlp.c_fc.weight\n",
      "h.2.mlp.c_fc.bias\n",
      "h.2.mlp.c_proj.weight\n",
      "h.2.mlp.c_proj.bias\n",
      "h.3.ln_1.weight\n",
      "h.3.ln_1.bias\n",
      "h.3.attn.attention.k_proj.weight\n",
      "h.3.attn.attention.v_proj.weight\n",
      "h.3.attn.attention.q_proj.weight\n",
      "h.3.attn.attention.out_proj.weight\n",
      "h.3.attn.attention.out_proj.bias\n",
      "h.3.ln_2.weight\n",
      "h.3.ln_2.bias\n",
      "h.3.mlp.c_fc.weight\n",
      "h.3.mlp.c_fc.bias\n",
      "h.3.mlp.c_proj.weight\n",
      "h.3.mlp.c_proj.bias\n",
      "h.4.ln_1.weight\n",
      "h.4.ln_1.bias\n",
      "h.4.attn.attention.k_proj.weight\n",
      "h.4.attn.attention.v_proj.weight\n",
      "h.4.attn.attention.q_proj.weight\n",
      "h.4.attn.attention.out_proj.weight\n",
      "h.4.attn.attention.out_proj.bias\n",
      "h.4.ln_2.weight\n",
      "h.4.ln_2.bias\n",
      "h.4.mlp.c_fc.weight\n",
      "h.4.mlp.c_fc.bias\n",
      "h.4.mlp.c_proj.weight\n",
      "h.4.mlp.c_proj.bias\n",
      "h.5.ln_1.weight\n",
      "h.5.ln_1.bias\n",
      "h.5.attn.attention.k_proj.weight\n",
      "h.5.attn.attention.v_proj.weight\n",
      "h.5.attn.attention.q_proj.weight\n",
      "h.5.attn.attention.out_proj.weight\n",
      "h.5.attn.attention.out_proj.bias\n",
      "h.5.ln_2.weight\n",
      "h.5.ln_2.bias\n",
      "h.5.mlp.c_fc.weight\n",
      "h.5.mlp.c_fc.bias\n",
      "h.5.mlp.c_proj.weight\n",
      "h.5.mlp.c_proj.bias\n",
      "h.6.ln_1.weight\n",
      "h.6.ln_1.bias\n",
      "h.6.attn.attention.k_proj.weight\n",
      "h.6.attn.attention.v_proj.weight\n",
      "h.6.attn.attention.q_proj.weight\n",
      "h.6.attn.attention.out_proj.weight\n",
      "h.6.attn.attention.out_proj.bias\n",
      "h.6.ln_2.weight\n",
      "h.6.ln_2.bias\n",
      "h.6.mlp.c_fc.weight\n",
      "h.6.mlp.c_fc.bias\n",
      "h.6.mlp.c_proj.weight\n",
      "h.6.mlp.c_proj.bias\n",
      "h.7.ln_1.weight\n",
      "h.7.ln_1.bias\n",
      "h.7.attn.attention.k_proj.weight\n",
      "h.7.attn.attention.v_proj.weight\n",
      "h.7.attn.attention.q_proj.weight\n",
      "h.7.attn.attention.out_proj.weight\n",
      "h.7.attn.attention.out_proj.bias\n",
      "h.7.ln_2.weight\n",
      "h.7.ln_2.bias\n",
      "h.7.mlp.c_fc.weight\n",
      "h.7.mlp.c_fc.bias\n",
      "h.7.mlp.c_proj.weight\n",
      "h.7.mlp.c_proj.bias\n",
      "h.8.ln_1.weight\n",
      "h.8.ln_1.bias\n",
      "h.8.attn.attention.k_proj.weight\n",
      "h.8.attn.attention.v_proj.weight\n",
      "h.8.attn.attention.q_proj.weight\n",
      "h.8.attn.attention.out_proj.weight\n",
      "h.8.attn.attention.out_proj.bias\n",
      "h.8.ln_2.weight\n",
      "h.8.ln_2.bias\n",
      "h.8.mlp.c_fc.weight\n",
      "h.8.mlp.c_fc.bias\n",
      "h.8.mlp.c_proj.weight\n",
      "h.8.mlp.c_proj.bias\n",
      "h.9.ln_1.weight\n",
      "h.9.ln_1.bias\n",
      "h.9.attn.attention.k_proj.weight\n",
      "h.9.attn.attention.v_proj.weight\n",
      "h.9.attn.attention.q_proj.weight\n",
      "h.9.attn.attention.out_proj.weight\n",
      "h.9.attn.attention.out_proj.bias\n",
      "h.9.ln_2.weight\n",
      "h.9.ln_2.bias\n",
      "h.9.mlp.c_fc.weight\n",
      "h.9.mlp.c_fc.bias\n",
      "h.9.mlp.c_proj.weight\n",
      "h.9.mlp.c_proj.bias\n",
      "h.10.ln_1.weight\n",
      "h.10.ln_1.bias\n",
      "h.10.attn.attention.k_proj.weight\n",
      "h.10.attn.attention.v_proj.weight\n",
      "h.10.attn.attention.q_proj.weight\n",
      "h.10.attn.attention.out_proj.weight\n",
      "h.10.attn.attention.out_proj.bias\n",
      "h.10.ln_2.weight\n",
      "h.10.ln_2.bias\n",
      "h.10.mlp.c_fc.weight\n",
      "h.10.mlp.c_fc.bias\n",
      "h.10.mlp.c_proj.weight\n",
      "h.10.mlp.c_proj.bias\n",
      "h.11.ln_1.weight\n",
      "h.11.ln_1.bias\n",
      "h.11.attn.attention.k_proj.weight\n",
      "h.11.attn.attention.v_proj.weight\n",
      "h.11.attn.attention.q_proj.weight\n",
      "h.11.attn.attention.out_proj.weight\n",
      "h.11.attn.attention.out_proj.bias\n",
      "h.11.ln_2.weight\n",
      "h.11.ln_2.bias\n",
      "h.11.mlp.c_fc.weight\n",
      "h.11.mlp.c_fc.bias\n",
      "h.11.mlp.c_proj.weight\n",
      "h.11.mlp.c_proj.bias\n",
      "ln_f.weight\n",
      "ln_f.bias\n"
     ]
    }
   ],
   "source": [
    "layers_to_freeze = 10\n",
    "\n",
    "for name, param in model.transformer.named_parameters():\n",
    "    print(name)\n",
    "\n",
    "    if layers_to_freeze:\n",
    "        if \"h.\" in name:\n",
    "            # Extract the layer number from the parameter name\n",
    "            layer_num = int(name.split(\".\")[1])\n",
    "        else:\n",
    "            layer_num = None\n",
    "            to_freeze = layer_num and layer_num < layers_to_freeze\n",
    "    else:\n",
    "        # if no layers are being frozen, set to_freeze to false for all layers\n",
    "        to_freeze = False\n",
    "\n",
    "    # Freeze the current parameter if it belongs to the embedding layer\n",
    "    # or if to_freeze is true for this layer\n",
    "    if name == \"wte.weight\" or to_freeze:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home2/response/.cache/huggingface/datasets/csv/default-40f43ffa42a8b736/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea5b661f7b642429468b9f643e90750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['0'],\n",
       "        num_rows: 1117\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainings_data = load_dataset('csv', data_files='./trainings_data/short_stories_1117.csv')\n",
    "\n",
    "trainings_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home2/response/.cache/huggingface/datasets/csv/default-40f43ffa42a8b736/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-32d8f24da1524aba.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['0', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1117\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"0\"], padding=\"max_length\", truncation=True, max_length=1024)\n",
    "\n",
    "tokenized_datasets = trainings_data.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_datasets[\"train\"] = tokenized_datasets[\"train\"].add_column(\"labels\", tokenized_datasets[\"train\"][\"input_ids\"])\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/response/.pyenv/versions/miniconda3-latest/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home2/response/.pyenv/versions/miniconda3-latest/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='420' max='420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [420/420 10:36, Epoch 2.99/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=420, training_loss=2.4462367466517856, metrics={'train_runtime': 640.4643, 'train_samples_per_second': 5.232, 'train_steps_per_second': 0.656, 'total_flos': 1750609966399488.0, 'train_loss': 2.4462367466517856, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output_transformers\", \n",
    "    # dataloader_num_workers=os.cpu_count(),\n",
    "    per_device_train_batch_size=4,\n",
    "    label_names=[\"input_ids\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leuk je te ontmoeten, waar ben je geweest?\n",
      "Ik zou je ongelofelijke dingen kunnen laten zien\n",
      "Magie, gekte, hemel, zonde\n",
      "Zag je daar en ik dacht\n",
      "“Oh mijn God, kijk naar dat gezicht\n",
      "Jij ziet eruit als mijn volgende fout\n",
      "Liefde is een spel, wil je spelen?”, zei ik\n",
      "ik keek om me heen maar zag niets meer\n",
      "de wereld was nog niet klaar voor jou\n",
      "en jij had het al gezien\n",
      "je bent hier gekomen omdat je zo van de ene op de andere dag in een grot terecht kwam\n",
      "het was alsof er iets mis ging met jouw ogen die dicht tegen elkaar aan zaten\n",
      "dat deed pijn\n",
      "maar wat gebeurde er dan\n",
      "wat doe je hier\n",
      "jij kijkt mij aan\n",
      "ik zie geen enkel teken van leven\n",
      "een klein beetje licht of donker kan alles zijn\n",
      "als ik jou nu vraag hoe heet je nou eigenlijk\n",
      "hoe heette je ook alweer\n",
      "of waarom heb je hem nooit eerder gezien\n",
      "hij heeft wel\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\n",
    "  'text-generation',\n",
    "  model,\n",
    "  tokenizer=tokenizer,\n",
    "  config=config,\n",
    "  device='cuda:0'\n",
    ")\n",
    "\n",
    "print(\n",
    "    generator(\n",
    "      \"Leuk je te ontmoeten, waar ben je geweest?\\n\"\n",
    "      \"Ik zou je ongelofelijke dingen kunnen laten zien\\n\"\n",
    "      \"Magie, gekte, hemel, zonde\\n\"\n",
    "      \"Zag je daar en ik dacht\\n\"\n",
    "      \"“Oh mijn God, kijk naar dat gezicht\\n\"\n",
    "      \"Jij ziet eruit als mijn volgende fout\\n\"\n",
    "      \"Liefde is een spel, wil je spelen?”, zei ik\\n\",\n",
    "      min_length=100,\n",
    "      max_length=200,\n",
    "      temperature=0.9,\n",
    "      top_k=50,\n",
    "      top_p=0.95,\n",
    "      repetition_penalty=1.2\n",
    "  )[0][\"generated_text\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
