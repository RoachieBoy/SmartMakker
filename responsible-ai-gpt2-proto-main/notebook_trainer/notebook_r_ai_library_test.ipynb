{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "# Model repository\n",
    "model_repo = \"yhavinga/gpt2-medium-dutch\"\n",
    "\n",
    "# Download the models\n",
    "config = AutoConfig.from_pretrained(model_repo)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_repo, config=config)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_repo, config=config)\n",
    "\n",
    "if config.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = config.eos_token_id\n",
    "else:\n",
    "    tokenizer.pad_token_id = config.pad_token_id\n",
    "\n",
    "# File path for saving\n",
    "save_model_path = \"./org_model\"\n",
    "\n",
    "# Save the original config, tokenizer and model\n",
    "config.save_pretrained(save_model_path)\n",
    "\n",
    "tokenizer.save_pretrained(save_model_path)\n",
    "model.save_pretrained(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from generative_ai.generators.hugging_face_generator import HuggingFaceGenerator\n",
    "\n",
    "generator = HuggingFaceGenerator(model_path=\"./../script_trainer/output/checkpoint\", deepspeed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from generative_ai.trainers import fine_tune_gpt_with_lightning_deepspeed\n",
    "\n",
    "fine_tune_gpt_with_lightning_deepspeed(\n",
    "    model_path=\"./org_model\",\n",
    "    output_path=\"./output\",\n",
    "    train_data=\"./lyrics_400.csv\",\n",
    "    epochs=1,\n",
    "    layers_to_freeze=20,\n",
    "    column=\"lyrics\",\n",
    "    learning_rate=1e-5,\n",
    "    batch_size=2\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T21:14:29.664329Z",
     "start_time": "2023-06-21T21:14:24.290091Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:For training, the BetterTransformer implementation for gpt2  architecture currently does not support padding as fused kernels do not support custom attention masks. Beware that passing padded batched training data may result in unexpected outputs.\n"
     ]
    }
   ],
   "source": [
    "from generative_ai.generators.hugging_face_generator import HuggingFaceGenerator\n",
    "\n",
    "generator = HuggingFaceGenerator(model_path=r\"../saved_local_models/GPT2 Medium mC4 - 1 Epoch 6000-L Dataset OneCycleLR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-21T21:14:53.501484Z",
     "start_time": "2023-06-21T21:14:49.372392Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/winand/DataspellProjects/responsible-ai-gpt2-proto/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betrapte mezelf er vandaag op dat ik jouw naam zong\n",
      "Je zei dat ik gek was en dat ben ik ook\n",
      "Maar ik weet dat zelfs als de werekd vergaat\n",
      "Dat ik nog steeds van je hou\n",
      "Ik wil niet meer alleen zijn\n",
      "Ik voel me zo goed met jou\n",
      "\n",
      "Je bent mijn grootste schat\n",
      "\n",
      "Ik heb zoveel van je geleerd\n",
      "Je hebt mij veranderd\n",
      "Ik kan nu zeggen wat ik echt wil\n",
      "Ik wil je nooit vergeten\n",
      "Ik zal altijd bij je blijven\n",
      "Ik hoop dat jij ooit weer terugkomt\n",
      "\n",
      "Ik zie het als een wonder\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 3.95 s, sys: 361 ms, total: 4.32 s\n",
      "Wall time: 4.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text = generator.generate_single_prompt(\n",
    "    \"Betrapte mezelf er vandaag op dat ik jouw naam zong\\n\"\n",
    "    \"Je zei dat ik gek was en dat ben ik ook\\n\"\n",
    "    \"Maar ik weet dat zelfs als de werekd vergaat\\n\"\n",
    "    \"Dat ik nog steeds van je hou\\n\",\n",
    "    max_length=120,\n",
    "    min_length=110,\n",
    "    temperature=.60,\n",
    "    top_k=60,\n",
    "    top_p=.60,\n",
    "    repetition_penalty=1.2,\n",
    "    no_repeat_ngram_size=0\n",
    ")\n",
    "\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
